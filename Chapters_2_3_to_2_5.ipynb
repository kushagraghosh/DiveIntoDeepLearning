{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYTr4vOJDh1z"
      },
      "source": [
        "# Dive into Deep Learning (DDL): Ch. 2.3-2.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Linear Algebra (2.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dKjpCjPhDRFd"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor(5.), tensor(6.), tensor(1.5000), tensor(9.))"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Scalars\n",
        "x = torch.tensor(3.0)\n",
        "y = torch.tensor(2.0)\n",
        "\n",
        "x + y, x * y, x / y, x**y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0, 1, 2])\n",
            "tensor(2)\n",
            "3\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "torch.Size([3])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = torch.arange(3) #Vetor\n",
        "print(x)\n",
        "print(x[2])\n",
        "print(len(x)) #Dimensionalty of the vector\n",
        "\n",
        "#order: number of axes, dimensionality: the number of components along a particular axis.\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0, 1],\n",
              "        [2, 3],\n",
              "        [4, 5]])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "A = torch.arange(6).reshape(3, 2)\n",
        "A #2nd order tensor with shape (m rows, n columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0, 2, 4],\n",
              "        [1, 3, 5]])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "A.T #Transpose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[True, True, True],\n",
              "        [True, True, True],\n",
              "        [True, True, True]])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "A = torch.tensor([[1, 2, 3], [2, 0, 4], [3, 4, 5]])\n",
        "A == A.T #Symmmetric matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[ 0,  1,  2,  3],\n",
              "         [ 4,  5,  6,  7],\n",
              "         [ 8,  9, 10, 11]],\n",
              "\n",
              "        [[12, 13, 14, 15],\n",
              "         [16, 17, 18, 19],\n",
              "         [20, 21, 22, 23]]])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Tensors for nth order arrays, arbritrary num of axes\n",
        "torch.arange(24).reshape(2, 3, 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[0., 1., 2.],\n",
              "         [3., 4., 5.]]),\n",
              " tensor([[ 0.,  2.,  4.],\n",
              "         [ 6.,  8., 10.]]))"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Elementwise addition\n",
        "A = torch.arange(6, dtype=torch.float32).reshape(2, 3)\n",
        "B = A.clone()  # Assign a copy of A to B by allocating new memory\n",
        "A, A + B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 0.,  1.,  4.],\n",
              "        [ 9., 16., 25.]])"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#elementwise product of two matrices (Hadamard product)\n",
        "A * B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[[ 2,  3,  4,  5],\n",
              "          [ 6,  7,  8,  9],\n",
              "          [10, 11, 12, 13]],\n",
              " \n",
              "         [[14, 15, 16, 17],\n",
              "          [18, 19, 20, 21],\n",
              "          [22, 23, 24, 25]]]),\n",
              " torch.Size([2, 3, 4]))"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Adding or multiplying a scalar and a tensor: each element of the tensor is added to (or multiplied by) the scalar\n",
        "\n",
        "a = 2\n",
        "X = torch.arange(24).reshape(2, 3, 4)\n",
        "a + X, (a * X).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([2, 3]), tensor(15.))"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = torch.arange(3, dtype=torch.float32)\n",
        "x, x.sum() #Sum of tensor's elements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[0., 1., 2.],\n",
              "         [3., 4., 5.]]),\n",
              " torch.Size([2, 3]),\n",
              " tensor(15.))"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "A, A.shape, A.sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([3., 5., 7.]) torch.Size([3])\n",
            "torch.Size([2, 3]) torch.Size([2])\n",
            "tensor(True)\n"
          ]
        }
      ],
      "source": [
        "print(A.sum(axis=0), A.sum(axis=0).shape) #Specify axis to reduce the tensor along the rows (axis 0)\n",
        "print(A.shape, A.sum(axis=1).shape) #Specify axis 1 to reduce column dimension (axis 1)\n",
        "print(A.sum(axis=[0, 1]) == A.sum())  # Same as A.sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([1.5000, 2.5000, 3.5000]), tensor([1.5000, 2.5000, 3.5000]))"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "A.mean(), A.sum() / A.numel() #Mean\n",
        "A.mean(axis=0), A.sum(axis=0) / A.shape[0] #Reduce a tensor along the rows (axis 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([ 3., 12.]) torch.Size([2])\n",
            "tensor([[ 3.],\n",
            "        [12.]]) torch.Size([2, 1])\n",
            "tensor([[0.0000, 0.3333, 0.6667],\n",
            "        [0.2500, 0.3333, 0.4167]])\n"
          ]
        }
      ],
      "source": [
        "#Non-reduction sum\n",
        "sum_A = A.sum(axis=1)\n",
        "print(sum_A, sum_A.shape)\n",
        "\n",
        "sum_A = A.sum(axis=1, keepdims=True)\n",
        "print(sum_A, sum_A.shape)\n",
        "\n",
        "print(A / sum_A) #Useful for brodcasting, A / sum of each row makes each row sum to 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0., 1., 2.],\n",
              "        [3., 5., 7.]])"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Cumulative sum across an axis (across row when axis=0)\n",
        "A.cumsum(axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0., 1., 2.]) tensor([1., 1., 1.]) tensor(3.)\n",
            "tensor(3.)\n"
          ]
        }
      ],
      "source": [
        "y = torch.ones(3, dtype = torch.float32)\n",
        "print(x, y, torch.dot(x, y)) #Dot product, inner product, sum over products of the elements at the same position\n",
        "print(torch.sum(x * y)) #Same as elementwise mult followed by sum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([2, 3]), torch.Size([3]), tensor([ 5., 14.]), tensor([ 5., 14.]))"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "A.shape, x.shape, torch.mv(A, x), A@x #Matrix vector multiplication"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[0., 1., 2.],\n",
              "         [3., 4., 5.]]),\n",
              " tensor([[1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1.]]),\n",
              " tensor([[ 3.,  3.,  3.,  3.],\n",
              "         [12., 12., 12., 12.]]),\n",
              " tensor([[ 3.,  3.,  3.,  3.],\n",
              "         [12., 12., 12., 12.]]))"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "B = torch.ones(3, 4)\n",
        "A, B, torch.mm(A, B), A@B #Matrix Matrix Mult"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(5.)"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Norm of a vector = how big it is (magnititue of a vector's componenets)\n",
        "u = torch.tensor([3.0, -4.0])\n",
        "torch.norm(u) #L2 norm is Euclidian length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(6.)"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.norm(torch.ones((4, 9))) #Frobenius norm (similar to l2 norm of matrix shaped vector, square root of sum of squares of a matrix's elems)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Automatic Differentation (2.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "#2.4 Calculus Content: \n",
        "\n",
        "# derivative is the instantaneous rate of change of f(x) with respect to x\n",
        "# slope of a function at a particular location.\n",
        "\n",
        "# concatenating partial derivatives of a multivariate function with respect to all its variables gets a vector that is called the Gradient of the function\n",
        "# Gradient difficult to calculate because we are workignw ith deeply nested dfuncigons, but we can use chain rule\n",
        "\n",
        "#evaluating the gradient (of y in terms of x) requires computing a \"vectorâ€“matrix product\" (between a matrix A that contains the derivates of vector u in terms of vector x with the derivate of y in terms of vector u)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0., 1., 2., 3.])"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# automatic differentiation (autograd): As we pass data through each successive function, the framework builds a computational graph that tracks how each value depends on others. \n",
        "# To calculate derivatives, automatic differentiation works backwards through this graph applying the chain rule. \n",
        "# The computational algorithm for applying the chain rule in this fashion is called backpropagation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([0., 1., 2., 3.], requires_grad=True), None)"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = torch.arange(4.0, requires_grad=True) #same as x.requires_grad_(True)\n",
        "x, x.grad # The gradient is None by default"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(28., grad_fn=<MulBackward0>)"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Calculate f(x)= 2 xT x and store it in y \n",
        "y = 2 * torch.dot(x, x)\n",
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([ 0.,  4.,  8., 12.])\n",
            "tensor([True, True, True, True])\n"
          ]
        }
      ],
      "source": [
        "y.backward() #take the gradient of y in terms of x\n",
        "print(x.grad) #access the gradient (an attribute of x)\n",
        "print(x.grad == 4 * x) #automatic gradient and expected result (4x being gradient) are identical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(6., grad_fn=<SumBackward0>)\n",
            "tensor([1., 1., 1., 1.])\n"
          ]
        }
      ],
      "source": [
        "x.grad.zero_()  # Reset the gradient\n",
        "y = x.sum()\n",
        "print(y)\n",
        "y.backward()\n",
        "print(x.grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0., 1., 4., 9.], grad_fn=<MulBackward0>)\n"
          ]
        }
      ],
      "source": [
        "x.grad.zero_()\n",
        "y = x * x\n",
        "print(y)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
